{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGvvHnA/wHJSzjbo94vUuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotetsu-n/pytorch_GANs_impl/blob/master/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3fk2DZe0ErU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "refered to \n",
        "https://github.com/YutaroOgawa/pytorch_advanced/blob/master/5_gan_generation/5-1-2_DCGAN.ipynb\n",
        "'''\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "def data_preparation_for_dcgan():\n",
        "    data_dir = \"./data/\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.mkdir(data_dir)\n",
        "\n",
        "    mnist = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n",
        "\n",
        "    def test_loading_mnist(mnist, visualize=False):\n",
        "        X = mnist.data\n",
        "        y = mnist.target\n",
        "\n",
        "        if visualize:\n",
        "            plt.imshow(X[0].reshape(28, 28), cmap='gray')\n",
        "        return X, y\n",
        "\n",
        "    X, y = test_loading_mnist(mnist)\n",
        "\n",
        "    data_dir_path = \"./data/img_78/\"\n",
        "    if not os.path.exists(data_dir_path):\n",
        "        os.mkdir(data_dir_path)\n",
        "\n",
        "    count7=0\n",
        "    count8=0\n",
        "    max_num=200\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        if (y[i] is \"7\") and (count7<max_num):\n",
        "            file_path=\"./data/img_78/img_7_\"+str(count7)+\".jpg\"\n",
        "            im_f=(X[i].reshape(28, 28)) \n",
        "            pil_img_f = Image.fromarray(im_f.astype(np.uint8))\n",
        "            pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)\n",
        "            pil_img_f.save(file_path)\n",
        "            count7+=1 \n",
        "        \n",
        "        if (y[i] is \"8\") and (count8<max_num):\n",
        "            file_path=\"./data/img_78/img_8_\"+str(count8)+\".jpg\"\n",
        "            im_f=(X[i].reshape(28, 28))\n",
        "            pil_img_f = Image.fromarray(im_f.astype(np.uint8))\n",
        "            pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)\n",
        "            pil_img_f.save(file_path)\n",
        "            count8+=1\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=20, image_size=64):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, image_size * 8,\n",
        "                               kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(image_size * 8),\n",
        "            nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(image_size*8, image_size*4,\n",
        "                               kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(image_size*4),\n",
        "            nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(image_size*4, image_size*2,\n",
        "                               kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(image_size*2),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(image_size*2, image_size,\n",
        "                               kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(image_size),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.last = nn.Sequential(\n",
        "            nn.ConvTranspose2d(image_size, 1,\n",
        "                               kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.layer1(z)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.last(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, z_dim=20, image_size=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, image_size, \n",
        "                      kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(image_size, image_size*2, \n",
        "                      kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(image_size*2, image_size*4, \n",
        "                      kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(image_size*4, image_size*8, \n",
        "                      kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.last(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def make_datapath_list():\n",
        "    train_img_list = list()\n",
        "\n",
        "    for img_idx in range(200):\n",
        "        img_path = \"./data/img_78/img_7_\" + str(img_idx)+'.jpg'\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "        img_path = \"./data/img_78/img_8_\" + str(img_idx)+'.jpg'\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "    return train_img_list\n",
        "\n",
        "class ImageTransform():\n",
        "    def __init__(self, mean, std):\n",
        "        self.data_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.data_transform(img)\n",
        "\n",
        "class GAN_Img_Dataset(data.Dataset):\n",
        "    def __init__(self, file_list, transform):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.file_list[index]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "        return img_transformed\n",
        "\n",
        "def check_generator():\n",
        "    G = Generator(z_dim=20, image_size=64)\n",
        "    input_z = torch.randn(1, 20)\n",
        "    input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "    fake_images = G(input_z)\n",
        "\n",
        "    img_transformed = fake_images[0][0].detach().numpy()\n",
        "    plt.imshow(img_transformed, 'gray')\n",
        "    plt.show()\n",
        "\n",
        "def check_discriminator():\n",
        "    G = Generator(z_dim=20, image_size=64)\n",
        "    D = Discriminator(z_dim=20, image_size=64)\n",
        "    input_z = torch.randn(1, 20)\n",
        "    input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "    fake_images = G(input_z)\n",
        "\n",
        "    d_out = D(fake_images)\n",
        "    print(nn.Sigmoid()(d_out))\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "def test_trained_result(model, iteration):\n",
        "    train_img_list = make_datapath_list()\n",
        "\n",
        "    mean = (0.5,)\n",
        "    std = (0.5,)\n",
        "    train_dataset = GAN_Img_Dataset(\n",
        "        file_list=train_img_list, transform=ImageTransform(mean, std))\n",
        "    \n",
        "    batch_size = 64\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    batch_size = 8\n",
        "    z_dim = 20\n",
        "    fixed_z = torch.randn(batch_size, z_dim)\n",
        "    fixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n",
        "\n",
        "    fake_images = model(fixed_z.to(device))\n",
        "\n",
        "    batch_iterator = iter(train_dataloader)\n",
        "    imges = next(batch_iterator)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 6))\n",
        "    for i in range(0, 5):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.imshow(imges[i][0].cpu().detach().numpy(), 'gray')\n",
        "\n",
        "        plt.subplot(2, 5, 5+i+1)\n",
        "        plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')\n",
        "    plt.savefig(\"images/{}.png\".format(iteration))\n",
        "\n",
        "def train_model(G, D, dataloader, num_epochs):\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print('currently use:', device)\n",
        "\n",
        "    # optimization\n",
        "    g_lr, d_lr = 0.0001, 0.0004\n",
        "    beta1, beta2 = 0.0, 0.9\n",
        "    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n",
        "    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "    z_dim = 20\n",
        "    mini_batch_size = 64\n",
        "\n",
        "    G.to(device)\n",
        "    D.to(device)\n",
        "\n",
        "    G.train()\n",
        "    D.train()\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    num_train_imgs = len(dataloader.dataset)\n",
        "    batch_size = dataloader.batch_size\n",
        "\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    os.makedirs('./images/', exist_ok=True)\n",
        "    sample_interval = 200\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        t_epoch_start = time.time()\n",
        "        epoch_g_loss = 0.0\n",
        "        epoch_d_loss = 0.0\n",
        "\n",
        "        print('----------')\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('----------')\n",
        "        print(' (train) ')\n",
        "\n",
        "        for imgs in dataloader:\n",
        "            if imgs.size()[0] == 1:\n",
        "                continue\n",
        "\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            mini_batch_size = imgs.size()[0]\n",
        "            label_real = torch.full((mini_batch_size,), 1).to(device)\n",
        "            label_fake = torch.full((mini_batch_size,), 0).to(device)\n",
        "\n",
        "            d_out_real = D(imgs)\n",
        "\n",
        "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
        "            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "            fake_imgs = G(input_z)\n",
        "            d_out_fake = D(fake_imgs)\n",
        "\n",
        "            # train discriminator\n",
        "            d_loss_real = criterion(d_out_real.view(-1), label_real)\n",
        "            d_loss_fake = criterion(d_out_fake.view(-1), label_fake)\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            d_optimizer.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # train generator\n",
        "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
        "            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "            fake_images = G(input_z)\n",
        "            d_out_fake = D(fake_images)\n",
        "            g_loss = criterion(d_out_fake.view(-1), label_real)\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            d_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            epoch_d_loss += d_loss.item()\n",
        "            epoch_g_loss += g_loss.item()\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            if iteration % sample_interval == 0:\n",
        "              test_trained_result(G, iteration)\n",
        "        \n",
        "        t_epoch_finish = time.time()\n",
        "        print('-------------')\n",
        "        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n",
        "            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "    return G, D\n",
        "\n",
        "def train():\n",
        "    train_img_list = make_datapath_list()\n",
        "\n",
        "    mean = (0.5,)\n",
        "    std = (0.5,)\n",
        "    train_dataset = GAN_Img_Dataset(\n",
        "        file_list=train_img_list, transform=ImageTransform(mean, std))\n",
        "    \n",
        "    batch_size = 64\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    batch_iterator = iter(train_dataloader)\n",
        "    imgs = next(batch_iterator)\n",
        "    print(imgs.size())\n",
        "\n",
        "    G = Generator(z_dim=20, image_size=64)\n",
        "    D = Discriminator(z_dim=20, image_size=64)\n",
        "\n",
        "    G.apply(weights_init)\n",
        "    D.apply(weights_init)\n",
        "\n",
        "    num_epochs = 200\n",
        "    G_update, D_update = train_model(\n",
        "        G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n",
        "\n",
        "    return G_update, D_update"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQOZEBH70V5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__=='__main__':\n",
        "    data_preparation_for_dcgan()\n",
        "    train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}