{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNc64XwkK+e5m7NwOT705bW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotetsu-n/pytorch_GANs_impl/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L_nDEMm4dg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_base.zip\n",
        "!wget http://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_extended.zip\n",
        "!mkdir ./dataset\n",
        "\n",
        "!yes Y | unzip ./CMP_facade_DB_base.zip -d ./dataset/base > /dev/null\n",
        "!yes Y | unzip ./CMP_facade_DB_extended.zip -d ./dataset/extended > /dev/null\n",
        "\n",
        "!mkdir ./dataset/train\n",
        "!mkdir ./dataset/train/img\n",
        "!mkdir ./dataset/train/label\n",
        "!cp ./dataset/base/base/*.jpg ./dataset/train/img/\n",
        "!cp ./dataset/base/base/*.png ./dataset/train/label/\n",
        "!cp ./dataset/extended/extended/*.jpg ./dataset/train/img/\n",
        "!cp ./dataset/extended/extended/*.png ./dataset/train/label/\n",
        "!rm ./output -r\n",
        "!mkdir ./output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh1srUMWCdyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm ./output -r\n",
        "# !mkdir ./output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxMe_xsC3zK",
        "colab_type": "text"
      },
      "source": [
        "# General"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aW9uYnm6-tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Refered to the followings to implement\n",
        "https://github.com/y-kamiya/machine-learning-samples/blob/master/python3/deep/pytorch/pix2pix.py\n",
        "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "if there are any problem to go public this code, please contact me\n",
        "'''\n",
        "import sys\n",
        "import argparse\n",
        "import os.path\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from PIL import Image\n",
        "\n",
        "class AlignedDataset(Dataset):\n",
        "    IMG_EXTENSIONS = ['.png', 'jpg']\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.imgdir = config.imgdir\n",
        "        self.labeldir = config.labeldir\n",
        "        self.img_paths = sorted(glob.glob(self.imgdir+'/*.jpg'))\n",
        "        print(self.imgdir+'/*.jpg')\n",
        "        self.label_paths = sorted(glob.glob(self.labeldir+'/*.png'))\n",
        "        self.direction = config.direction\n",
        "\n",
        "    @classmethod\n",
        "    def is_image_file(self, fname):\n",
        "        return any(fname.endswith(ext) for ext in self.IMG_EXTENSIONS)\n",
        "\n",
        "    @classmethod\n",
        "    def __make_dataset(self, dir):\n",
        "        images = []\n",
        "        assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "        for root, _, fnames in sorted(os.walk(dir)):\n",
        "            for fname in fnames:\n",
        "                if self.is_image_file(fname):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    images.append(path)\n",
        "        return images\n",
        "\n",
        "    def __transform(self, param):\n",
        "        list = []\n",
        "\n",
        "        load_size = self.config.load_size\n",
        "        list.append(transforms.Resize([load_size, load_size], Image.BICUBIC))\n",
        "\n",
        "        (x, y) = param['crop_pos']\n",
        "        crop_size = self.config.crop_size\n",
        "        list.append(transforms.Lambda(lambda img: img.crop((x, y, x + crop_size, y + crop_size))))\n",
        "\n",
        "        if param['flip']:\n",
        "            list.append(transforms.Lambda(lambda img: img.transpose(Image.FLIP_LEFT_RIGHT)))\n",
        "\n",
        "        list += [transforms.ToTensor(),\n",
        "                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "\n",
        "        return transforms.Compose(list)\n",
        "\n",
        "    def __transform_param(self):\n",
        "        x_max = self.config.load_size - self.config.crop_size\n",
        "        x = random.randint(0, np.maximum(0, x_max))\n",
        "        y = random.randint(0, np.maximum(0, x_max))\n",
        "\n",
        "        flip = random.random() > 0.5\n",
        "\n",
        "        return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.img_paths[index]\n",
        "        label_path = self.label_paths[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        label = Image.open(label_path).convert('RGB')\n",
        "\n",
        "        param = self.__transform_param()\n",
        "        transform = self.__transform(param)\n",
        "        if self.direction == 'A2B':\n",
        "          A = transform(img)\n",
        "          B = transform(label)\n",
        "          A_path = img_path\n",
        "          B_path = label_path\n",
        "        else:\n",
        "          A = transform(label)\n",
        "          B = transform(img)\n",
        "          A_path = label_path\n",
        "          B_path = img_path\n",
        "\n",
        "        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOZHmQUF-uaC",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avv-xjZA-tXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.down0 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.down1 = self.__down(64, 128)\n",
        "        self.down2 = self.__down(128, 256)\n",
        "        self.down3 = self.__down(256, 512)\n",
        "        self.down4 = self.__down(512, 512)\n",
        "        self.down5 = self.__down(512, 512)\n",
        "        self.down6 = self.__down(512, 512)\n",
        "        self.down7 = self.__down(512, 512, use_norm=False)\n",
        "\n",
        "        self.up7 = self.__up(512, 512)\n",
        "        self.up6 = self.__up(1024, 512, use_dropout=True)\n",
        "        self.up5 = self.__up(1024, 512, use_dropout=True)\n",
        "        self.up4 = self.__up(1024, 512, use_dropout=True)\n",
        "        self.up3 = self.__up(1024, 256)\n",
        "        self.up2 = self.__up(512, 128)\n",
        "        self.up1 = self.__up(256, 64)\n",
        "\n",
        "        self.up0 = nn.Sequential(\n",
        "            self.__up(128, 3, use_norm=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def __down(self, input, output, use_norm=True):\n",
        "        layer = [\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(input, output, kernel_size=4, stride=2, padding=1),\n",
        "        ]\n",
        "        if use_norm:\n",
        "            layer.append(nn.BatchNorm2d(output))\n",
        "\n",
        "        return nn.Sequential(*layer)\n",
        "\n",
        "    def __up(self, input, output, use_norm=True, use_dropout=False):\n",
        "        layer = [\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(input, output, kernel_size=4, stride=2, padding=1),\n",
        "        ]\n",
        "        if use_norm:\n",
        "            layer.append(nn.BatchNorm2d(output))\n",
        "\n",
        "        if use_dropout:\n",
        "            layer.append(nn.Dropout(0.5))\n",
        "\n",
        "        return nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.down0(x)\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.down3(x2)\n",
        "        x4 = self.down4(x3)\n",
        "        x5 = self.down5(x4)\n",
        "        x6 = self.down6(x5)\n",
        "        x7 = self.down7(x6)\n",
        "\n",
        "        y7 = self.up7(x7)\n",
        "        y6 = self.up6(self.concat(x6, y7))\n",
        "        y5 = self.up5(self.concat(x5, y6))\n",
        "        y4 = self.up4(self.concat(x4, y5))\n",
        "        y3 = self.up3(self.concat(x3, y4))\n",
        "        y2 = self.up2(self.concat(x2, y3))\n",
        "        y1 = self.up1(self.concat(x1, y2))\n",
        "        y0 = self.up0(self.concat(x0, y1))\n",
        "\n",
        "        return y0\n",
        "\n",
        "    def concat(self, x, y):\n",
        "        return torch.cat([x, y], dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NbuCrJH-udu",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyyDx7yV-5qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            self.__layer(64, 128),\n",
        "            self.__layer(128, 256),\n",
        "            self.__layer(256, 512, stride=1),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def __layer(self, input, output, stride=2):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(input, output, kernel_size=4, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(output),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7T3dqIR_Fk5",
        "colab_type": "text"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cq6uSFA_HCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GANLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GANLoss, self).__init__()\n",
        "\n",
        "        self.register_buffer('real_label', torch.tensor(1.0))\n",
        "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def __call__(self, prediction, is_real):\n",
        "        if is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "\n",
        "        return self.loss(prediction, target_tensor.expand_as(prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz7TsOgN-_A7",
        "colab_type": "text"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVtfuUg__Aq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pix2Pix():\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.netG = Generator().to(self.config.device)\n",
        "        self.netG.apply(self.__weights_init)\n",
        "        if self.config.generator != None:\n",
        "            self.netG.load_state_dict(torch.load(self.config.generator, map_location=self.config.device_name), strict=False)\n",
        "\n",
        "        self.netD = Discriminator().to(self.config.device)\n",
        "        self.netD.apply(self.__weights_init)\n",
        "        if self.config.discriminator != None:\n",
        "            self.netD.load_state_dict(torch.load(self.config.discriminator, map_location=self.config.device_name), strict=False)\n",
        "\n",
        "        self.optimizerG = optim.Adam(self.netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.optimizerD = optim.Adam(self.netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.criterionGAN = GANLoss().to(self.config.device)\n",
        "        self.criterionL1 = nn.L1Loss()\n",
        "\n",
        "        self.schedulerG = optim.lr_scheduler.LambdaLR(self.optimizerG, self.__modify_learning_rate)\n",
        "        self.schedulerD = optim.lr_scheduler.LambdaLR(self.optimizerD, self.__modify_learning_rate)\n",
        "\n",
        "        self.training_start_time = time.time()\n",
        "        self.append_log(config)\n",
        "        self.append_log(self.netG)\n",
        "        self.append_log(self.netD)\n",
        "\n",
        "        # for log\n",
        "        self.epoch_lossG, self.epoch_lossG_GAN, self.epoch_lossG_L1 = 0.0, 0.0, 0.0,\n",
        "        self.epoch_lossD, self.epoch_lossD_fake, self.epoch_lossD_real = 0.0, 0.0, 0.0\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        self.schedulerG.step()\n",
        "        self.schedulerD.step()\n",
        "\n",
        "    def __modify_learning_rate(self, epoch):\n",
        "        if self.config.epochs_lr_decay_start < 0:\n",
        "            return 1.0\n",
        "\n",
        "        delta = max(0, epoch - self.config.epochs_lr_decay_start) / float(self.config.epochs_lr_decay + 1)\n",
        "        return max(0.0, 1.0 - delta)\n",
        "\n",
        "    def __weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def set_requires_grad(self, nets, requires_grad=False):\n",
        "        if not isinstance(nets, list):\n",
        "            nets = [nets]\n",
        "        for net in nets:\n",
        "            if net is not None:\n",
        "                for param in net.parameters():\n",
        "                    param.requires_grad = requires_grad\n",
        "\n",
        "    def train(self, data):\n",
        "        self.realA = data['A'].to(self.config.device)\n",
        "        self.realB = data['B'].to(self.config.device)\n",
        "\n",
        "        self.fakeB = self.netG(self.realA)\n",
        "\n",
        "        # Discriminator\n",
        "        self.set_requires_grad(self.netD, True)\n",
        "        self.optimizerD.zero_grad()\n",
        "\n",
        "        fakeAB = torch.cat((self.realA, self.fakeB), dim=1)\n",
        "        pred_fake = self.netD(fakeAB.detach())\n",
        "        self.lossD_fake = self.criterionGAN(pred_fake, False)\n",
        "\n",
        "        realAB = torch.cat((self.realA, self.realB), dim=1)\n",
        "        pred_real = self.netD(realAB)\n",
        "        self.lossD_real = self.criterionGAN(pred_real, True)\n",
        "\n",
        "        self.lossD = (self.lossD_fake + self.lossD_real) * 0.5\n",
        "\n",
        "        self.lossD.backward()\n",
        "        self.optimizerD.step()\n",
        "\n",
        "        # Generator\n",
        "        self.set_requires_grad(self.netD, False)\n",
        "        self.optimizerG.zero_grad()\n",
        "        # with torch.no_grad():\n",
        "        #     pred_fake = self.netD(fakeAB)\n",
        "        fakeAB = torch.cat((self.realA, self.fakeB), dim=1)\n",
        "        pred_fake = self.netD(fakeAB)\n",
        "        self.lossG_GAN = self.criterionGAN(pred_fake, True)\n",
        "        \n",
        "        self.lossG_L1 = self.criterionL1(self.fakeB, self.realB) * self.config.lambda_L1\n",
        "\n",
        "        self.lossG = self.lossG_GAN + self.lossG_L1\n",
        "\n",
        "        self.lossG.backward()\n",
        "        self.optimizerG.step()\n",
        "\n",
        "        # for log\n",
        "        # self.last_fakeB = self.fakeB\n",
        "        self.epoch_lossG += self.lossG\n",
        "        self.epoch_lossG_GAN += self.lossG_GAN\n",
        "        self.epoch_lossG_L1 += self.lossG_L1\n",
        "        self.epoch_lossD += self.lossD\n",
        "        self.epoch_lossD_real += self.lossD_real\n",
        "        self.epoch_lossD_fake += self.lossD_fake\n",
        "\n",
        "    def save(self, epoch):\n",
        "        output_dir = self.config.output_dir\n",
        "        torch.save(self.netG.state_dict(), '{}/pix2pix_G_epoch_{}'.format(output_dir, epoch))\n",
        "        torch.save(self.netD.state_dict(), '{}/pix2pix_D_epoch_{}'.format(output_dir, epoch))\n",
        "\n",
        "    def save_image(self, epoch):\n",
        "        output_image = torch.cat([self.realA, self.fakeB, self.realB], dim=3)\n",
        "        vutils.save_image(output_image,\n",
        "                '{}/pix2pix_epoch_{}.png'.format(self.config.output_dir, epoch),\n",
        "                normalize=True)\n",
        "\n",
        "    def print_epochloss(self, epoch, iteration):\n",
        "        elapsed_time = time.time() - self.training_start_time\n",
        "        \n",
        "        losses = [self.epoch_lossG, self.epoch_lossG_GAN, self.epoch_lossG_L1, \n",
        "                  self.epoch_lossD, self.epoch_lossD_real, self.epoch_lossD_fake]\n",
        "        for i, loss in enumerate(losses):\n",
        "            losses[i] /= iteration\n",
        "        message = 'epoch: {}, time: {:.3f}, lossG: {:.3f}, lossG_GAN: {:.3f}, lossG_L1: {:.3f}, lossD: {:.3f}, lossD_real: {:.3f}, lossD_fake: {:.3f}, lr: {:.5f}'.format(\n",
        "            epoch, elapsed_time, *losses, self.optimizerG.param_groups[0]['lr'])\n",
        "        print(message)\n",
        "\n",
        "        self.append_log(message)\n",
        "        self.epoch_lossG, self.epoch_lossG_GAN, self.epoch_lossG_L1 = 0.0, 0.0, 0.0,\n",
        "        self.epoch_lossD, self.epoch_lossD_fake, self.epoch_lossD_real = 0.0, 0.0, 0.0\n",
        "\n",
        "    def append_log(self, message):\n",
        "        log_file = '{}/pix2pix.log'.format(self.config.output_dir)\n",
        "        with open(log_file, \"a\") as log_file:\n",
        "            log_file.write('{}\\n'.format(message))  # save the message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfPQtJoA7qS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(add_help=True)\n",
        "    parser.add_argument('--epochs', type=int, default=200, help='epoch count')\n",
        "    parser.add_argument('--save_data_interval', type=int, default=10, help='save data interval epochs')\n",
        "    parser.add_argument('--save_image_interval', type=int, default=1, help='save image interval epochs')\n",
        "    parser.add_argument('--log_interval', type=int, default=1, help='log interval epochs')\n",
        "    parser.add_argument('--batch_size', type=int, default=8, help='epoch count')\n",
        "    parser.add_argument('--load_size', type=int, default=300, help='scale images to this size')\n",
        "    parser.add_argument('--crop_size', type=int, default=256, help='then crop to this size')\n",
        "    parser.add_argument('--cpu', action='store_true', help='use cpu')\n",
        "    parser.add_argument('--imgdir', default='./dataset/train/img', help='path to the data directory')\n",
        "    parser.add_argument('--labeldir', default='./dataset/train/label', help='path to the data directory')\n",
        "    parser.add_argument('--output_dir', default='./output', help='output directory')\n",
        "    parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
        "    parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for L1 loss')\n",
        "    parser.add_argument('--generator', help='file path to data for generator')\n",
        "    parser.add_argument('--discriminator', help='file path to data for discriminator')\n",
        "    parser.add_argument('--epochs_lr_decay', type=int, default=100, help='epochs to delay lr to zero')\n",
        "    parser.add_argument('--epochs_lr_decay_start', type=int, default=0, help='epochs to lr delay start')\n",
        "    parser.add_argument('--direction', type=str, default='B2A', help='direction of conversion')\n",
        "    args = parser.parse_args(args=['--imgdir', './dataset/train/img'])\n",
        "    print(args)\n",
        "\n",
        "    is_cpu = args.cpu or not torch.cuda.is_available()\n",
        "    args.device_name = \"cpu\" if is_cpu else \"cuda:0\"\n",
        "    args.device = torch.device(args.device_name)\n",
        "\n",
        "    model = Pix2Pix(args)\n",
        "    dataset = AlignedDataset(args)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        iteration = 0\n",
        "\n",
        "        for i, data in enumerate(dataloader):\n",
        "            model.train(data)\n",
        "            iteration += len(data)\n",
        "\n",
        "        if epoch % args.save_data_interval == 0:\n",
        "            model.save(epoch)\n",
        "\n",
        "        if epoch % args.save_image_interval == 0:\n",
        "            model.save_image(epoch)\n",
        "\n",
        "        if epoch % args.log_interval == 0:\n",
        "            model.print_epochloss(epoch, iteration)\n",
        "\n",
        "        model.update_learning_rate()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}