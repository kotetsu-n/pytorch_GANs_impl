{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cycleGAN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOy1BvwWYQOP1xk8pRWM4I5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotetsu-n/pytorch_GANs_impl/blob/master/cycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQG2MDe1VJBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROtbXM7LVQzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip horse2zebra.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VBtWloKKmjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./dataset\n",
        "!mkdir ./dataset/trainA ./dataset/trainB ./dataset/testA ./dataset/testB\n",
        "!mv ./horse2zebra/trainA ./dataset/trainA/\n",
        "!mv ./horse2zebra/trainB ./dataset/trainB/\n",
        "!mv ./horse2zebra/testA ./dataset/testA/\n",
        "!mv ./horse2zebra/testB ./dataset/testB/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3joIXS05SNl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "To compose this notebook, \n",
        "I refererd to https://github.com/arnab39/cycleGAN-PyTorch\n",
        "If there are any problems, please contact me.\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import itertools\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "def init_weights(net, init_type='normal', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            init.normal(m.weight.data, 0.0, gain)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal(m.weight.data, 1.0, gain)\n",
        "            init.constant(m.bias.data, 0.0)\n",
        "\n",
        "    print('Network initialized with weights sampled from N(0,0.02).')\n",
        "    net.apply(init_func)\n",
        "\n",
        "def init_network(net, gpu_id):\n",
        "    if gpu_id > -1:\n",
        "        assert(torch.cuda.is_available())\n",
        "        net.cuda(gpu_id)\n",
        "        # net = torch.nn.DataParallel(net, gpu_ids)\n",
        "    init_weights(net)\n",
        "    return net\n",
        "\n",
        "def conv_norm_lrelu(in_dim, out_dim, kernel_size, stride=1, padding=0,\n",
        "                    norm_layer = nn.BatchNorm2d, bias = False):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, bias = bias),\n",
        "        norm_layer(out_dim), nn.LeakyReLU(0.2,True))\n",
        "\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, input_ch, base_ch=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_bias=False):\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        dis_layers = [nn.Conv2d(input_ch, base_ch, kernel_size=4, stride=2, padding=1),\n",
        "                           nn.LeakyReLU(0.2, True)]\n",
        "        mult_coef = 1\n",
        "        mult_coef_prev = 1\n",
        "        for n in range(1, n_layers):\n",
        "            mult_coef_prev = mult_coef\n",
        "            mult_coef = min(2**n, 8)\n",
        "            dis_layers += [nn.Conv2d(base_ch * mult_coef_prev, base_ch * mult_coef, \n",
        "                                    kernel_size=4, stride=2, padding=1, bias = use_bias),\n",
        "                          norm_layer(base_ch * mult_coef),\n",
        "                          nn.LeakyReLU(0.2, True)]\n",
        "        mult_coef_prev = mult_coef\n",
        "        mult_coef = min(2**n_layers, 8)\n",
        "        dis_layers += [nn.Conv2d(base_ch * mult_coef_prev, base_ch * mult_coef,\n",
        "                                kernel_size=4, stride=1, padding=1, bias=use_bias),\n",
        "                                norm_layer(base_ch * mult_coef),\n",
        "                                nn.LeakyReLU(0.2, True)]\n",
        "        dis_layers += [nn.Conv2d(base_ch * mult_coef, 1, kernel_size=4, stride=1, padding=1)]\n",
        "\n",
        "        self.dis_layers = nn.Sequential(*dis_layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.dis_layers(input)\n",
        "\n",
        "class PixelDiscriminator(nn.Module):\n",
        "    def __init__(self, input_ch, base_ch=64, norm_layer=nn.BatchNorm2d, use_bias=False):\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        dis_layers = [\n",
        "            nn.Conv2d(input_ch, base_ch, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(base_ch, base_ch * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_lAyer(base_ch * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(base_ch * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "        \n",
        "        self.dis_layers = nn.Sequential(*dis_layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.dis_layers(input)\n",
        "\n",
        "def def_discriminator(input_ch, base_ch, network_type='n_layers', n_layers=3, norm='batch', gpu_id=0):\n",
        "    if norm == 'batch':\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        use_bias = False\n",
        "    elif norm == 'instance':\n",
        "        norm_layer = nn.InstanceNorm2d\n",
        "        use_bias = True\n",
        "    \n",
        "    if network_type == 'n_layers':\n",
        "        discriminator = NLayerDiscriminator(input_ch, base_ch, n_layers, norm_layer=norm_layer, use_bias=use_bias)\n",
        "    elif network_type == 'pixel':\n",
        "        discriminator = PixelDiscriminator(input_ch, base_ch, norm_layer=norm_layer, use_bias=use_bias)\n",
        "\n",
        "    return init_network(discriminator, gpu_id)\n",
        "\n",
        "def get_norm_layer(norm_type='instance'):\n",
        "    if norm_type == 'batch':\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
        "    return norm_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYH28wOBTg2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    def __init__(self, outer_nc, inner_nc, input_ch=None, submodule=None, \n",
        "                 outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        use_bias = norm_layer = nn.InstanceNorm2d\n",
        "        if input_ch == None:\n",
        "            input_ch = outer_nc\n",
        "        downconv = nn.Conv2d(input_ch, inner_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc*2, outer_nc, kernel_size=4, stride=2, padding=1)\n",
        "            down = [downconv]\n",
        "            up = [nn.ReLU(True), upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "            down = [nn.LeakyReLU(0.2, True), downconv]\n",
        "            up = [nn.ReLU(True), upconv, norm_layer(outer_nc)]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc*2, outer_nc, kernel_size=4,\n",
        "                                        stride=2, padding=1, bias=use_bias)\n",
        "            down = [nn.LeakyReLU(0.2, True), downconv, norm_layer(inner_nc)]\n",
        "            up = [nn.ReLU(True), upconv, norm_layer(outer_nc)]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([x, self.model(x)], 1)\n",
        "                \n",
        "class UnetGenerator(nn.Module):\n",
        "    def __init__(self, input_ch, output_ch, num_downs, base_ch=64,\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "\n",
        "        unet_block = UnetSkipConnectionBlock(base_ch*8, base_ch*8, submodule=None, \n",
        "                                             norm_layer=norm_layer, innermost=True)\n",
        "        for i in range(num_downs - 5):\n",
        "            unet_block = UnetSkipConnectionBlock(base_ch*8, base_ch*8,\n",
        "                                                 submodule=unet_block,\n",
        "                                                 norm_layer=norm_layer,\n",
        "                                                 use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(base_ch*4, base_ch*8, submodule=unet_block, \n",
        "                                             norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(base_ch*2, base_ch*4, \n",
        "                                             submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(base_ch, base_ch*2, \n",
        "                                             submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(output_ch, base_ch, input_ch=input_ch,\n",
        "                                             submodule=unet_block, outermost=True,\n",
        "                                             norm_layer=norm_layer)\n",
        "        self.unet_model = unet_block\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.unet_model(input)\n",
        "\n",
        "def def_generator(input_ch, output_ch, base_ch, network_type='unet128', \n",
        "               norm='batch', use_dropout=False, gpu_id=0):\n",
        "    if norm == 'batch':\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "    elif norm == 'instance':\n",
        "        norm_layer = nn.InstanceNorm2d\n",
        "\n",
        "    if network_type == 'unet128':\n",
        "        generator = UnetGenerator(input_ch, output_ch, 7, base_ch, norm_layer=norm, use_dropout=use_dropout)\n",
        "    elif network_type == 'unet256':\n",
        "        generator = UnetGenerator(input_ch, output_ch, 8, base_ch, norm_layer=norm, use_dropout=use_dropout)\n",
        "    \n",
        "    return init_network(generator, gpu_id)\n",
        "\n",
        "class LambdaLR():\n",
        "    def __init__(self, epochs, offset, decay_epoch):\n",
        "        self.epochs = epochs\n",
        "        self.offset = offset\n",
        "        self.decay_epoch = decay_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_epoch)/(self.epochs - self.decay_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQYjszn3TnVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class sample_from_pool(object):\n",
        "        \"\"\"\n",
        "        sample from pooled items\n",
        "\n",
        "        Args:\n",
        "            max_elements (int): max number of pool items\n",
        "        \"\"\"        \n",
        "        def __init__(self, max_elements=50):\n",
        "            self.max_elements = max_elements\n",
        "            self.cur_elements = 0\n",
        "            self.items = []\n",
        "\n",
        "        def __call__(self, in_items):\n",
        "            return_items = []\n",
        "            for in_item in in_items:\n",
        "                if self.cur_elements < self.max_elements:\n",
        "                    self.items.append(in_item)\n",
        "                    self.cur_elements += 1\n",
        "                    return_items.append(in_item)\n",
        "                else:\n",
        "                    if np.random.ranf() > 0.5:\n",
        "                        idx = np.random.randint(0, self.max_elements)\n",
        "                        tmp = copy.copy(self.items[idx])\n",
        "                        self.items[idx] = in_item\n",
        "                        return_items.append(tmp)\n",
        "                    else:\n",
        "                        return_items.append(in_item)\n",
        "            return return_items\n",
        "\n",
        "class cycleGAN(object):\n",
        "    \"\"\"\n",
        "        A (source domain), B (target domain).\n",
        "        Generators: G_A: A -> B; G_B: B -> A.\n",
        "        Discriminators: D_A: G_A(A) vs. B; D_B: G_B(B) vs. A.\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        self.G_A = def_generator(input_ch=3, output_ch=3, base_ch=args.base_ch, network_type=args.generator_net,\n",
        "                                 norm='batch', use_dropout=args.dropout, gpu_id=args.gpu_id)\n",
        "        self.G_B = def_generator(input_ch=3, output_ch=3, base_ch=args.base_ch, network_type=args.generator_net,\n",
        "                                 norm='batch', use_dropout=args.dropout, gpu_id=args.gpu_id)\n",
        "        \n",
        "        self.D_A = def_discriminator(input_ch=3, base_ch=args.base_ch, network_type=args.discriminator_net,\n",
        "                                    n_layers=3, norm=args.norm, gpu_id=args.gpu_id)\n",
        "        self.D_B = def_discriminator(input_ch=3, base_ch=args.base_ch, network_type=args.discriminator_net,\n",
        "                                    n_layers=3, norm=args.norm, gpu_id=args.gpu_id)\n",
        "\n",
        "        # losses\n",
        "        self.MSE = nn.MSELoss()\n",
        "        self.L1 = nn.L1Loss()\n",
        "\n",
        "        print(self.G_A)\n",
        "\n",
        "        # optimiezers\n",
        "        self.g_optimizer = torch.optim.Adam(itertools.chain(self.G_A.parameters(), self.G_B.parameters()),\n",
        "                                            lr=args.lr, betas=(0.5, 0.999))\n",
        "        self.d_optimizer = torch.optim.Adam(itertools.chain(self.D_A.parameters(), self.D_B.parameters()),\n",
        "                                            lr=args.lr, betas=(0.5, 0.999))\n",
        "\n",
        "        self.g_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.g_optimizer,\n",
        "                                                                lr_lambda=LambdaLR(args.epochs, 0, args.decay_epoch).step)\n",
        "        self.d_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.d_optimizer,\n",
        "                                                                lr_lambda=LambdaLR(args.epochs, 0, args.decay_epoch).step)\n",
        "\n",
        "        # reserve\n",
        "        self.training_dirs = None\n",
        "        self.a_loader = None\n",
        "        self.b_loader = None\n",
        "        self.a_fake_sample = None\n",
        "        self.b_fake_sample = None\n",
        "\n",
        "        if not os.path.isdir(args.checkpoint_dir):\n",
        "            os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "        \n",
        "        try:\n",
        "            ckpt = torch.load(f'{args.checkpoint_dir}/latest.ckpt')\n",
        "            self.start_epoch = ckpt['epoch']\n",
        "            self.D_A.load_state_dict(ckpt['D_A'])\n",
        "            self.D_B.load_state_dict(ckpt['D_B'])\n",
        "            self.G_A.load_state_dict(ckpt['G_A'])\n",
        "            self.G_B.load_state_dict(ckpt['G_B'])\n",
        "            self.d_optimizer.load_state_dict(ckpt['d_optimizer'])\n",
        "            self.g_optimizer.load_state_dict(ckpt['g_optimizer'])\n",
        "        except:\n",
        "            print('No checkpoints!!')\n",
        "            self.start_epoch = 0\n",
        "\n",
        "    def prepare_data(self, args):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.Resize((args.load_height, args.load_width)),\n",
        "            transforms.RandomCrop((args.crop_height, args.crop_width)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "        self.training_dirs = {'trainA': os.path.join(args.dataset_dir, 'trainA'), \n",
        "                                     'trainB': os.path.join(args.dataset_dir, 'trainB')}\n",
        "    \n",
        "        self.a_loader = torch.utils.data.DataLoader(\n",
        "            datasets.ImageFolder(self.training_dirs['trainA'], transform=transform),\n",
        "            batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
        "        self.b_loader = torch.utils.data.DataLoader(\n",
        "            datasets.ImageFolder(self.training_dirs['trainB'], transform=transform),\n",
        "            batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "        self.a_fake_sample = sample_from_pool()\n",
        "        self.b_fake_sample = sample_from_pool()\n",
        "\n",
        "    def set_grad_status(self, nets, requires_grad=False):\n",
        "        for net in nets:\n",
        "            for param in net.parameters():\n",
        "                param.requires_grad = requires_grad\n",
        "        \n",
        "    def train(self, args, test=None, output_interval=None):\n",
        "        self.prepare_data(args)\n",
        "        for epoch in range(self.start_epoch, args.epochs):\n",
        "            lr = self.g_optimizer.param_groups[0]['lr']\n",
        "            print(f'current learning rate = {lr}')\n",
        "\n",
        "            epoch_gen_loss = 0\n",
        "            epoch_dis_loss= 0\n",
        "\n",
        "            for i, (a_real, b_real) in enumerate(zip(self.a_loader, self.b_loader)):\n",
        "                step = epoch * min(len(self.a_loader), len(self.b_loader)) + i + 1\n",
        "\n",
        "                # ------ GENERATOR ------\n",
        "                self.set_grad_status([self.D_A, self.D_B], False)  ### really needed?\n",
        "                self.g_optimizer.zero_grad()\n",
        "\n",
        "                a_real = Variable(a_real[0]).cuda()\n",
        "                b_real = Variable(b_real[0]).cuda()\n",
        "                \n",
        "                a_fake = self.G_B(b_real)\n",
        "                b_fake = self.G_A(a_real)\n",
        "                \n",
        "                a_recon = self.G_B(b_fake)\n",
        "                b_recon = self.G_A(a_fake)\n",
        "\n",
        "                a_idt = self.G_B(a_real)\n",
        "                b_idt = self.G_A(b_real)\n",
        "\n",
        "                # identity losses L1(idt <-> real)\n",
        "                a_idt_loss = self.L1(a_idt, a_real) * args.lamda * args.idt_coef\n",
        "                b_idt_loss = self.L1(b_idt, b_real) * args.lamda * args.idt_coef\n",
        "\n",
        "                # adversarial Losses MSE(pred(fake) <-> 1.0)\n",
        "                pred_a_fake = self.D_A(a_fake)\n",
        "                pred_b_fake = self.D_B(b_fake)\n",
        "\n",
        "                real_label = Variable(torch.ones(pred_a_fake.size())).cuda()\n",
        "\n",
        "                a_gen_loss = self.MSE(pred_a_fake, real_label)\n",
        "                b_gen_loss = self.MSE(pred_b_fake, real_label)\n",
        "\n",
        "                # cycle consistency losses L1(recon <-> real)\n",
        "                a_cycle_loss = self.L1(a_recon, a_real) * args.lamda\n",
        "                b_cycle_loss = self.L1(b_recon, b_real) * args.lamda\n",
        "\n",
        "                ### total generators losses\n",
        "                gen_loss = a_gen_loss + b_gen_loss + a_cycle_loss + b_cycle_loss + a_idt_loss + b_idt_loss\n",
        "\n",
        "                # update generators\n",
        "                gen_loss.backward()\n",
        "                self.g_optimizer.step()\n",
        "\n",
        "\n",
        "                # ------ DISCRIMINATOR ------ \n",
        "                self.set_grad_status([self.D_A, self.D_B], True) ### really needed?\n",
        "                self.d_optimizer.zero_grad()\n",
        "\n",
        "                # sample from histry of generated images\n",
        "                a_fake = Variable(torch.Tensor(self.a_fake_sample([a_fake.cpu().data.numpy()])[0])).cuda()\n",
        "                b_fake = Variable(torch.Tensor(self.b_fake_sample([b_fake.cpu().data.numpy()])[0])).cuda()\n",
        "\n",
        "                # forward\n",
        "                pred_a_real = self.D_A(a_real)\n",
        "                pred_b_real = self.D_B(b_real)\n",
        "                pred_a_fake = self.D_A(a_fake)\n",
        "                pred_b_fake = self.D_B(b_fake)\n",
        "                real_label = Variable(torch.ones(pred_a_real.size())).cuda()\n",
        "                fake_label = Variable(torch.zeros(pred_a_fake.size())).cuda()\n",
        "\n",
        "                # Discriminator losses\n",
        "                dis_a_real_loss = self.MSE(pred_a_real, real_label)\n",
        "                dis_b_real_loss = self.MSE(pred_b_real, real_label)\n",
        "                dis_a_fake_loss = self.MSE(pred_a_fake, fake_label)\n",
        "                dis_b_fake_loss = self.MSE(pred_b_fake, fake_label)\n",
        "\n",
        "                # Total discriminators losses\n",
        "                dis_a_loss = (dis_a_real_loss + dis_a_fake_loss)*0.5\n",
        "                dis_b_loss = (dis_b_real_loss + dis_b_fake_loss)*0.5\n",
        "\n",
        "                # Update\n",
        "                dis_a_loss.backward()\n",
        "                dis_b_loss.backward()\n",
        "                self.d_optimizer.step()\n",
        "\n",
        "                log_str = f'Epoch: {epoch}, ({i+1}/{min(len(self.a_loader), len(self.b_loader))}) \\\n",
        "                                Gen loss:{gen_loss:.4f}, Dis loss:{0.5*(dis_a_loss+dis_b_loss):.4f}'\n",
        "                print(\"\\r\"+log_str, end=\"\")\n",
        "                \n",
        "                epoch_gen_loss += gen_loss\n",
        "                epoch_dis_loss += 0.5*(dis_a_loss+dis_b_loss)\n",
        "\n",
        "            epoch_gen_loss /= min(len(self.a_loader), len(self.b_loader))\n",
        "            epoch_dis_loss /= min(len(self.a_loader), len(self.b_loader))\n",
        "            print(f'Epoch: {epoch}, Gen loss:{epoch_gen_loss:.4f}, Dis loss:{epoch_dis_loss:.4f}')\n",
        "\n",
        "            torch.save({'epoch':epoch+1,\n",
        "                        'D_A': self.D_A.state_dict(),\n",
        "                        'D_B': self.D_B.state_dict(),\n",
        "                        'G_A': self.G_A.state_dict(),\n",
        "                        'G_B': self.G_B.state_dict(),\n",
        "                        'd_optimizer': self.g_optimizer.state_dict(),\n",
        "                        'g_optimizer': self.g_optimizer.state_dict()}, \n",
        "                        f'{args.checkpoint_dir}/lates.ckpt')\n",
        "\n",
        "            # update learning late\n",
        "            self.g_lr_scheduler.step()\n",
        "            self.d_lr_scheduler.step()\n",
        "\n",
        "            if test is not None:\n",
        "                if epoch % output_interval == 0:\n",
        "                    test(args, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWRq1aayUNVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, epoch=-1):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize((args.crop_height,args.crop_width)),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    test_dirs = {'testA': os.path.join(args.dataset_dir, 'testA'),\n",
        "                 'testB': os.path.join(args.dataset_dir, 'testB')}\n",
        "\n",
        "    a_loader = torch.utils.data.DataLoader(\n",
        "        datasets.ImageFolder(test_dirs['testA'], transform=transform),\n",
        "        batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
        "    b_loader = torch.utils.data.DataLoader(\n",
        "        datasets.ImageFolder(test_dirs['testB'], transform=transform),\n",
        "        batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    G_A = def_generator(input_ch=3, output_ch=3, base_ch=args.base_ch, network_type=args.generator_net, \n",
        "                        norm=args.norm, use_dropout= args.dropout, gpu_id=args.gpu_id)\n",
        "    G_B = def_generator(input_ch=3, output_ch=3, base_ch=args.base_ch, network_type=args.generator_net, \n",
        "                        norm=args.norm, use_dropout= args.dropout, gpu_id=args.gpu_id)\n",
        "\n",
        "    try:\n",
        "        ckpt = torch.load(f'{args.checkpoint_dir}/latest.ckpt')\n",
        "        self.G_A.load_state_dict(ckpt['G_A'])\n",
        "        self.G_B.load_state_dict(ckpt['G_B'])\n",
        "    except:\n",
        "        print('No checkpoints!!')\n",
        "\n",
        "    ### Generation\n",
        "    a_real = Variable(iter(a_loader).next()[0], requires_grad=True).cuda()\n",
        "    b_real = Variable(iter(b_loader).next()[0], requires_grad=True).cuda()\n",
        "\n",
        "    G_A.eval()\n",
        "    G_B.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        a_fake = G_B(b_real)\n",
        "        b_fake = G_A(a_real)\n",
        "        a_recon = G_B(b_fake)\n",
        "        b_recon = G_A(a_fake)\n",
        "        a_idt = G_B(a_real)\n",
        "        b_idt = G_A(b_real)\n",
        "        \n",
        "    pic = (torch.cat([a_real, b_fake, a_recon, a_idt, b_real, a_fake, b_recon, b_idt], dim=0).data + 1) / 2.0\n",
        "\n",
        "    if not os.path.isdir(args.results_dir):\n",
        "        os.makedirs(args.results_dir)\n",
        "\n",
        "    save_name = f'/sample_{epoch}.jpg' if epoch > -1 else '/sample.jpg'\n",
        "    torchvision.utils.save_image(pic, args.results_dir+save_name, nrow=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oW1KjGlSWUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='cycleGAN PyTorch')\n",
        "    parser.add_argument('--epochs', type=int, default=200)\n",
        "    parser.add_argument('--decay_epoch', type=int, default=100)\n",
        "    parser.add_argument('--batch_size', type=int, default=1)\n",
        "    parser.add_argument('--lr', type=float, default=.0002)\n",
        "    parser.add_argument('--load_height', type=int, default=286)\n",
        "    parser.add_argument('--load_width', type=int, default=286)\n",
        "    parser.add_argument('--gpu_id', type=str, default=0)\n",
        "    parser.add_argument('--crop_height', type=int, default=256)\n",
        "    parser.add_argument('--crop_width', type=int, default=256)\n",
        "    parser.add_argument('--lamda', type=int, default=10)\n",
        "    parser.add_argument('--idt_coef', type=float, default=0.5)\n",
        "    parser.add_argument('--training', action='store_true')\n",
        "    parser.add_argument('--testing', action='store_true')\n",
        "    parser.add_argument('--results_dir', type=str, default='./results')\n",
        "    parser.add_argument('--dataset_dir', type=str, default='./dataset')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints/horse2zebra')\n",
        "    parser.add_argument('--output_interval', type=int, default=5)\n",
        "    parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization')\n",
        "    parser.add_argument('--dropout', action='store_true', help='dropout for the generator')\n",
        "    parser.add_argument('--base_ch', type=int, default=64, help='# of gen filters in first conv layer')\n",
        "    parser.add_argument('--generator_net', type=str, default='unet256')\n",
        "    parser.add_argument('--discriminator_net', type=str, default='n_layers')\n",
        "    args = parser.parse_args(args=['--training', '--testing', '--output_interval', '1'])\n",
        "    \n",
        "    if args.training:\n",
        "        model = cycleGAN(args)\n",
        "        if args.testing and args.output_interval > 0:\n",
        "            print('training & testing')\n",
        "            model.train(args, test, args.output_interval)\n",
        "        else:\n",
        "            print('training')\n",
        "            model.train(args)\n",
        "\n",
        "    elif args.testing:\n",
        "        print('testing')\n",
        "        test(args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWpZedYYsqzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}